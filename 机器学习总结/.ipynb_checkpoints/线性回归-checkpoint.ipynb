{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as py\n",
    "# 生成数据，生成一条直线，并添加高斯噪声\n",
    "# 用生成的数据进行分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法(LSM) VS 极大似然估计(MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LSM vs MLE](https://www.cnblogs.com/monoSLAM/p/5257589.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无监督：通常用于已知观察值X，和某分布$G(\\theta)$，比如高斯分布，做参数估计\n",
    "- 有监督：二分类逻辑回归参数估计(李航书上，P79），个人感觉用LSM(NG,第六章)比较容易理解\n",
    "- 无监督：高斯混合模型(EM,期望极大化算法，先求期望，再极大化期望)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在已有的观察X下，若假设X服从某分布$G(\\theta)$，那么可以使用极大似然估计来估计参数$\\theta$，使得$P(X|\\theta)$极大;比如当$X ～ N(\\mu, \\delta^2)$时，可以写出X的对数似然函数$L(\\theta)=\\sum lnP(xi|\\theta)$。然后通过对参数求导等于0，求得极大似然估计参数值，如果$X ～ N(\\mu, \\delta^2)$时，可以得到参数估计形式："
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKkAAABSCAYAAAAvgPT1AAALsklEQVR4Ae2djU3kOhDHw9MrAF0JiApWdHAIXQErSkBUgCgBUcGJEhAVoJOuAXQVIEpASFfAPv3yPMuscRLbibN21pFWm038MfP3eD7sSfaoqcciEXh/f988PDw07+/vzfn5efP29tb8+fOnubm5aU5PT48WyXRlqiwEHh8fN4bizcvLS3t+c3Oz+fnzp1wvi6FK7TIRQDjPz8+3Qrler7cCWxLH/wwRy8w7Pj7eMjpUvt7PB4Ffv361pl4o4vfZ2dmyTD0zr2majTIdwm/9LgABtKiY+ufn5w3j+fr6uuG8APKHSTRmogroMFTZltCmnkAKIV2MT3p3d9dq0Kurq2XMuGzFqBIWhQAzDhPPx5xHtVMrjUMAs6w14bjWyq79JXBibY1jvV433759W5aTXchYYZIZB9Y26+FAYLVatVp0Mb6Lg8cSLqFJT05OqrvVNM2/9oDJ7D05ObFv1d8ZI8Ay4cfHR7NarbCATkrZfeJgjCkrB2P99vZWlNVsNaksXQgjqb/xf6v2/kQ5VJOaZcKNb7AL3tQx2nq7VPVJQd5nswsp23WAxqoCa3h1XbZpQoUUkTICGrwOCu6+wr0P0f1i7vdBxNXVVXN7e9u8vLw0mJ7Ly8tZTQ8aXNwczf/Z2dnstOj+Q88fHh6OiCkuLy9JLNn4Br63t7dHKIrQ/vZZfnZNyiyW2Vzq/vLUAxajSaGBeiwfmgDYmyyE2rtwBgVnF1IA4iOzuTTAph4zXB4x3UxeI3je3Yh/Knh6Vyyo4KRCima094r1nrLGZe5gTfpmUDH5DKr4xPyWvW4pV9I3tKNRbexjecgNo0mFFKC0JjBaMpvdLAZR6OObpRwJ3Djf18SJFSZdj8gdHsZaphCMwG5qDZ40cII5nHidCU66GMGRr1OvQZdzTKGs+cm1rm/6ur+/7wzEWC8U+iR4ksDt4+PjiOCp65iSDtMHCqLv6OTDVenx8ZHUPII/123vayEY8RSA4OfdQUTBVpP+/v17CLDBpplRxuxsyzKwxt/aXsvlxEVvLrTF0gFPU+YA5ILRZOaeCNNeoMcEiTmNBT5VPRe9qfqao13MPDyNNfea1j6MCPJspaTrxp4nNfc8+KXNJWCxZYdJwNeLNfsJzKysK+5ksuOuXFxcdJrYFHTEDqSrHmYekz/GtdLt6vGT64IRyuj6+voIJWTKdeImdcd8T6JJIZ6gSQcemAoceYizNewYgmPrQqNFT0sb7UG3BFSx7e+zHhrNjMEoMkIwAi807agOHZWTaVICJJIdSDkT844GNUkQrRZ10DPrJbQ62gafDa2O1kE40f4EZqU+D4TZBcg+K2ADjYCBh611QzBirNk9vL6+tpuf9vePHz82379/H+3HMKNEOKelcLmtYSYRMCwOGgxrg0sRotGpF+MXxtSxR0JM/RxWklm4Y6ZtYjx/B4Hr2eaii6lJvcUfgfUddIQ5NJJnYiCg9DMWXNqI2SGL6Xe0kGo/JoaAQ66Du6EFDQHSfn0XNgibbyRPWdo0gjl6vLtoSnl9NNHM/ClmZkomc20bTSQ+JTRKYDdErxHsduywhCEfzPRQ+7ndHy2kuTFUEj0Im2hO8S8x45yXxMeUtH55EG/KxmtbcQjIqoKsMbNSEhKpx/VaVq2qScsar5Zagq4YFysm2MoBniqkOYxCIA0EQ4FV2g0VBLv6pKHI1fKzIoAmzV1IO33S4+PjWcGqncUjwGrAFIvx8RSkrbmzLYrJYBuMQ3Is03ZfWx+LAMt9OrGD9niosS/f9u7urtzxNTseSZIExg5Grd+NACY7NrGjOHPPUgcHSQL1KAcBSewoh+IwSrfmXkw9viiZQckzWcLorKV7EHh6emrfWcAuE2M3pGSKNfeyj3vIOxs9cpD1LcYuJrGDsTaP8kTVnxUUI5hRi8GzElo7O0wEREB1UsNhIlG5zhYBosKY7bRsGaqELQ6BI5YgSGTg0Y6np6ekD08tDj0HQwSgRNusU4Ipz/LXf6JzABV6CUElMlzyrkUoJrHlx2bXx/Z7EPWMT+r9qMJBgBLJZGx2fWR3h1XNZIGPfgjvsFD7yi0TXgeivtn1X1uqV0BgJ8GEhWAOfKp6xCPAzh3+KAerJ5zjUpmVlPiGa83/AcU3zT11K/ex0g/SEUjh6/s+8Zk7b/ugbyeaZ7afnp4KHTv35GL9rgikRIDAkxURVkd4kQivWHf110b6JuJ33V/0NfjWmnDRzGbGHFZH+/JYINbwtwkmNr1I8VyH7+xJTQ8mGX9c3lOaur/a/i4C/LEHrzqSA1++Kz6a9RmnrtkjhM79jSatPvncqH/2py04WpQEmE5N+lkt7VnI7ElLSVzrLC+Zl7Al/Sc6JjNahiz8OErzqYXFYiXJfjkaFOonQkhBfH5+dhI+qyaFAtfscVI2w8VQTSpPM5iUt0EKjbCJtt6+CGKoom/7Q+3kcB8MhvjBH+1bsptdSDVwmFottPreHOehQgpNBvBeUF20EyQMDRb1KCdvNXG1U+I1BFAHSZoHzS/jsXdzr4lj9pBVrlW+vp/reep/osPM8691ufKv6cKUuwJPkpj0Hz7wRhbXc1mS+8D7YhFQTL7r2IsmtWePi7A5rsVoUuiiHhshLuD76Mb09d1n0Pn0lSn1HmMuQgkPgiE4yqcrjbQtYCrMwj+E8mHA6LfLDKQmBhMkphsaQjEwgE+an8uabSgdqXGaqn1cGKxnTHutkE7lA6EFzJLOdnYwSyTpwgzAzr2u2RPDzNx1DOjB/mkXnYKT6z6TAnzBiwkmv6FhX4ItNPjS1Mefi2e5NpmQGh+z1ZBoSbRCyQIoAA19MykBf8iUD7XD/a5BRChFEPmmnNHkbR2tZLg+B+4hNCnekbfeI1ngxAzH6SXbX3YNYIK3a4w9MMl9b+jQ7Q/9I54pOwRUUNBCoGMCBU1K8DnCd3Fx0e5h25VZm5UAUwIVCUzsf/Jj50bu2e1M+TuEJumX8WFCudZMpUwyIUUYSVmTd2zSIUzIa3yEgJjvrqSDmLZMnSAhHOqH94uiuXhsJNWhhU5SA7si4T4BgD6ExFd5IPBdKw0hNI3FZbS5F/NjE4IGnMPs2P3O/RszT5Q/1txTv8vca57oq2sFgAAwNjjRfYSe99Gk2/Lhr1OT/v37V7cVdI75QY3bmgQzyL37+/ug9uzCCcy93cWo32z5weuQBhvqhPpDg4ggY53QcHLgVrEOieDql5mNpUfaH/ruo2moru/90ZqUjuwkDWazOPa+hJRYDj4Rkqlody1B0b4Ir9Gg2/4w3RJQQQPnoWu3MbSH0CTtQ6uPlu/UpNJQ7DeaBKFEo6I90S7yLvjYNnOvJ+u7Ie+3R4jQhF1abr1et7695p3y4IkAgy9YM+D4/wSUGmeC1jn+pS6EJuEFXxo+unxpKef6nkSTuhrO9RrmCQHDX0YjoJ1wKbRGGqKdej5awW5nqA60IYx2Pd/fWDTa6PJZfdtJUW6Mhj84IVVuyDaRA4H1HViEOVSQEBwE1CeQpAyaMkZQqBuzexbTV0gdUQYhdXTZgxNSmEcItKAhQD6CgbChEfjWILrOKUObRjCDcEazu9os8Ro4jOUnCLwSQXLRjLYRn5L7Epi4yuprRrBbzCQpwvfbDi51u/Y5A+ur2e26uf2GD59J3Uf3QQopwiaaU/xLzDjnfWDVe+kR2Hk5RPru8u5BomLZJSP6DInU8+ZuQdQZE3QQa5pTDFts8kZMsDUFvYtowzi04twvgqeUTMT4VvhkBE8hPmlKHopr2/hl2x2N4hgohGA0aRVSv8H64pPil61WqzZjqQYN/SCyGjC0GN/fQr3rg4BzW5SAge02tqwwZ11bdj4dLLUMJlsnb8Dnov+JLseBNFp0Z4E7Rzr3SRMmO3Zrr5p7/5H7Yu6lKksvr6+vbcJC9Z0Eld1vSd7YvVp/TY2A09xLJ/J4AsssmP567CJA9g6vCWKHCXwW+090u2zXXyUhEJu8gSslS305Jn+UNAaV1opAFgj8BwEIRaix1DmKAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是有偏估计，无偏估计$\\sigma^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_{i} - \\bar x)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通常出现在线性回归中。\n",
    "- 属于有监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta)=\\sum_{x=1}^{n}(\\theta*x_i-y_i)^2$,真实值和预测值相差值的平方和最小；通常可以使用求伪逆矩阵的方式求得解析解或者通过梯度下降法求得参数$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 等价关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 做线性回归时，如果噪声服从高斯分布，此时LSM可以由MLE推导，说明此时二者等价\n",
    "- Ridge回归可以用Gaussian分布和最大后验估计(MAP)解释  \n",
    "- LASSO回归可以用Lapace分布和最大后验估计(MAP)解释  \n",
    "[知乎](https://www.zhihu.com/question/20447622)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE vs MAP vs 贝叶斯估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE认为参数是确定未知的  \n",
    "贝叶斯认为参数是概率出现的  \n",
    "MAP是二者的折衷P(D)很难计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](image/MLE_MAP_Bayes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用MLE推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/逻辑回归MLE.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LSM推导"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
